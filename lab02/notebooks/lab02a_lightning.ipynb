{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlH0lCOttCs5"
      },
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUPRHaeetRnT"
      },
      "source": [
        "# Lab 02a: PyTorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bry3Hr-PcgDs"
      },
      "source": [
        "### What You Will Learn\n",
        "\n",
        "- The core components of a PyTorch Lightning training loop: `LightningModule`s and `Trainer`s.\n",
        "- Useful quality-of-life improvements offered by PyTorch Lightning: `LightningDataModule`s, `Callback`s, and `Metric`s\n",
        "- How we use these features in the FSDL codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs0LXXlCU6Ix"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkQiK7lkgeXm"
      },
      "source": [
        "If you're running this notebook on Google Colab,\n",
        "the cell below will run full environment setup.\n",
        "\n",
        "It should take about three minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "input_dir = '/home/suhdas/development/research/FSDL/fsdl-text-recognizer-2022-labs_understanding/lab02'\n",
        "# Set the current working directory to the specific folder\n",
        "os.chdir(input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVx7C7H0PIZC"
      },
      "outputs": [],
      "source": [
        "# lab_idx = 2\n",
        "\n",
        "# if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "#     # path management for Python\n",
        "#     pythonpath, = !echo $PYTHONPATH\n",
        "#     if \".\" not in pythonpath.split(\":\"):\n",
        "#         pythonpath = \".:\" + pythonpath\n",
        "#         %env PYTHONPATH={pythonpath}\n",
        "#         !echo $PYTHONPATH\n",
        "\n",
        "#     # get both Colab and local notebooks into the same state\n",
        "#     !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
        "#     import bootstrap\n",
        "\n",
        "#     # change into the lab directory\n",
        "#     bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
        "\n",
        "#     # allow \"hot-reloading\" of modules\n",
        "#     %load_ext autoreload\n",
        "#     %autoreload 2\n",
        "#     # needed for inline plots in some contexts\n",
        "#     %matplotlib inline\n",
        "\n",
        "#     bootstrap.run = False  # change to True re-run setup\n",
        "    \n",
        "!pwd\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZN4bGgsgWc_"
      },
      "source": [
        "# Why Lightning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP8iJW_bg7IC"
      },
      "source": [
        "PyTorch is a powerful library for executing differentiable\n",
        "tensor operations with hardware acceleration\n",
        "and it includes many neural network primitives,\n",
        "but it has no concept of \"training\".\n",
        "At a high level, an `nn.Module` is a stateful function with gradients\n",
        "and a `torch.optim.Optimizer` can update that state using gradients,\n",
        "but there's no pre-built tools in PyTorch to iteratively generate those gradients from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7gIA-Efy91E"
      },
      "source": [
        "So the first thing many folks do in PyTorch is write that code --\n",
        "a \"training loop\" to iterate over their `DataLoader`,\n",
        "which in pseudocode might look something like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ewkWrwzDA8"
      },
      "source": [
        "```python\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = some_loss_function(targets, outputs)\n",
        "    \n",
        "    optimizer.zero_gradients()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYUtiJWize82"
      },
      "source": [
        "This is a solid start, but other needs immediately arise.\n",
        "You'll want to run your model on validation and test data,\n",
        "which need their own `DataLoader`s.\n",
        "Once finished, you'll want to save your model --\n",
        "and for long-running jobs, you probably want\n",
        "to save checkpoints of the training process\n",
        "so that it can be resumed in case of a crash.\n",
        "For state-of-the-art model performance in many domains,\n",
        "you'll want to distribute your training across multiple nodes/machines\n",
        "and across multiple GPUs within those nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0untumvjy5fm"
      },
      "source": [
        "That's just the tip of the iceberg, and you want\n",
        "all those features to work for lots of models and datasets,\n",
        "not just the one you're writing now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNPpi4OZjMbu"
      },
      "source": [
        "You don't want to write all of this yourself.\n",
        "\n",
        "So unless you are at a large organization that has a dedicated team\n",
        "for building that \"framework\" code,\n",
        "you'll want to use an existing library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnQuyVqUjJy8"
      },
      "source": [
        "PyTorch Lightning is a popular framework on top of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7ecipNFTgZDt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/2.4.0/'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "version = pl.__version__\n",
        "\n",
        "docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/\"  # version can also be latest, stable\n",
        "docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE82xoEikWkh"
      },
      "source": [
        "At its core, PyTorch Lightning provides\n",
        "\n",
        "1. the `pl.Trainer` class, which organizes and executes your training, validation, and test loops, and\n",
        "2. the `pl.LightningModule` class, which links optimizers to models and defines how the model behaves during training, validation, and testing.\n",
        "\n",
        "Both of these are kitted out with all the features\n",
        "a cutting-edge deep learning codebase needs:\n",
        "- flags for switching device types and distributed computing strategy\n",
        "- saving, checkpointing, and resumption\n",
        "- calculation and logging of metrics\n",
        "\n",
        "and much more.\n",
        "\n",
        "Importantly these features can be easily\n",
        "added, removed, extended, or bypassed\n",
        "as desired, meaning your code isn't constrained by the framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuJUDmCeT3RK"
      },
      "source": [
        "In some ways, you can think of Lightning as a tool for \"organizing\" your PyTorch code,\n",
        "as shown in the video below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wTt0TBs5TZpm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"720\"\n",
              "            height=\"720\"\n",
              "            src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7263091a83a0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython.display as display\n",
        "\n",
        "\n",
        "display.IFrame(src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\",\n",
        "               width=720, height=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwpDn5GWn_X"
      },
      "source": [
        "That's opposed to the other way frameworks are designed,\n",
        "to provide abstractions over the lower-level library\n",
        "(here, PyTorch).\n",
        "\n",
        "Because of this \"organize don't abstract\" style,\n",
        "writing PyTorch Lightning code involves\n",
        "a lot of over-riding of methods --\n",
        "you inherit from a class\n",
        "and then implement the specific version of a general method\n",
        "that you need for your code,\n",
        "rather than Lightning providing a bunch of already\n",
        "fully-defined classes that you just instantiate,\n",
        "using arguments for configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXiUcQwan39S"
      },
      "source": [
        "# The `pl.LightningModule`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3FffD5Vn6we"
      },
      "source": [
        "The first of our two core classes,\n",
        "the `LightningModule`,\n",
        "is like a souped-up `torch.nn.Module` --\n",
        "it inherits all of the `Module` features,\n",
        "but adds more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0QWwSStJTP28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "issubclass(pl.LightningModule, torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1wiBVSTuHNT"
      },
      "source": [
        "To demonstrate how this class works,\n",
        "we'll build up a `LinearRegression` model dynamically,\n",
        "method by method.\n",
        "\n",
        "For this example we hard code lots of the details,\n",
        "but the real benefit comes when the details are configurable.\n",
        "\n",
        "In order to have a realistic example as well,\n",
        "we'll compare to the actual code\n",
        "in the `BaseLitModel` we use in the codebase\n",
        "as we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fPARncfQ3ohz"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.lit_models import BaseLitModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myyL0vYU3z0a"
      },
      "source": [
        "A `pl.LightningModule` is a `torch.nn.Module`,\n",
        "so the basic definition looks the same:\n",
        "we need `__init__` and `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-c0ylFO9rW_t"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
        "\n",
        "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
        "        self.model = torch.nn.Linear(in_features=1, out_features=1)\n",
        "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
        "\n",
        "    # optionally, define a forward method\n",
        "    def forward(self, xs):\n",
        "        return self.model(xs)  # we like to just call the model's forward method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY1yoGTy6CBu"
      },
      "source": [
        "But just the minimal definition for a `torch.nn.Module` isn't sufficient.\n",
        "\n",
        "If we try to use the class above with the `Trainer`, we get an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tBWh_uHu5rmU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error:\n",
            "\tNo `training_step()` method defined. Lightning `Trainer` expects as minimum a\n",
            "\t`training_step()`, `train_dataloader()` and `configure_optimizers()` to be\n",
            "\tdefined.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/suhdas/miniconda3/envs/TSTCC_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        }
      ],
      "source": [
        "import logging  # import some stdlib components to control what's display\n",
        "import textwrap\n",
        "import traceback\n",
        "\n",
        "\n",
        "try:  # try using the LinearRegression LightningModule defined above\n",
        "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)  # hide some info for now\n",
        "\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # we'll explain how the Trainer works in a bit\n",
        "    # trainer = pl.Trainer(gpus=int(torch.cuda.is_available()), max_epochs=1)\n",
        "    trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1 if torch.cuda.is_available() else \"auto\",\n",
        "    max_epochs=1\n",
        "    )\n",
        "    trainer.fit(model=model)  \n",
        "\n",
        "except pl.utilities.exceptions.MisconfigurationException as error:\n",
        "    print(\"Error:\", *textwrap.wrap(str(error), 80), sep=\"\\n\\t\")  # show the error without raising it\n",
        "\n",
        "finally:  # bring back info-level logging\n",
        "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ni7xe5CgUt"
      },
      "source": [
        "The error message says we need some more methods.\n",
        "\n",
        "Two of them are mandatory components of the `LightningModule`: `.training_step` and `.configure_optimizers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37BXP7nAoBik"
      },
      "source": [
        "#### `.training_step`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah9MjWz2plFv"
      },
      "source": [
        "The `training_step` method defines,\n",
        "naturally enough,\n",
        "what to do during a single step of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plWEvWG_zRia"
      },
      "source": [
        "Roughly, it gets used like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RbxZ4idy-C5"
      },
      "source": [
        "```python\n",
        "\n",
        "# pseudocode modified from the Lightning documentation\n",
        "\n",
        "# put model in train mode\n",
        "model.train()\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    # run the train step\n",
        "    loss = training_step(batch)\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cemh_hGJ53nL"
      },
      "source": [
        "Effectively, it maps a batch to a loss value,\n",
        "so that PyTorch can backprop through that loss.\n",
        "\n",
        "The `.training_step` for our `LinearRegression` model is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X8qW2VRRsPI2"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def training_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    xs, ys = batch  # unpack the batch\n",
        "    outs = self(xs)  # apply the model\n",
        "    loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "LinearRegression.training_step = training_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2e8m3BRCIx6"
      },
      "source": [
        "If you've written PyTorch code before, you'll notice that we don't mention devices\n",
        "or other tensor metadata here -- that's handled for us by Lightning, which is a huge relief."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkvNpfwqpns5"
      },
      "source": [
        "You can additionally define\n",
        "a `validation_step` and a `test_step`\n",
        "to define the model's behavior during\n",
        "validation and testing loops.\n",
        "\n",
        "You're invited to define these steps\n",
        "in the exercises at the end of the lab.\n",
        "\n",
        "Inside this step is also where you might calculate other\n",
        "values related to inputs, outputs, and loss,\n",
        "like non-differentiable metrics (e.g. accuracy, precision, recall).\n",
        "\n",
        "So our `BaseLitModel`'s got a slightly more complex `training_step` method,\n",
        "and the details of the forward pass are deferred to `._run_on_batch` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xpBkRczao1hr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mBaseLitModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m\n",
            "Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\n",
            "logger.\n",
            "\n",
            "Args:\n",
            "    batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\n",
            "    batch_idx: The index of this batch.\n",
            "    dataloader_idx: The index of the dataloader that produced this batch.\n",
            "        (only if multiple dataloaders used)\n",
            "\n",
            "Return:\n",
            "    - :class:`~torch.Tensor` - The loss tensor\n",
            "    - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of\n",
            "      automatic optimization.\n",
            "    - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for\n",
            "      multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\n",
            "      the loss is not required.\n",
            "\n",
            "In this step you'd normally do the forward pass and calculate the loss for a batch.\n",
            "You can also do fancier things like multiple forward passes or something model specific.\n",
            "\n",
            "Example::\n",
            "\n",
            "    def training_step(self, batch, batch_idx):\n",
            "        x, y, z = batch\n",
            "        out = self.encoder(x)\n",
            "        loss = self.loss(out, x)\n",
            "        return loss\n",
            "\n",
            "To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:\n",
            "\n",
            ".. code-block:: python\n",
            "\n",
            "    def __init__(self):\n",
            "        super().__init__()\n",
            "        self.automatic_optimization = False\n",
            "\n",
            "\n",
            "    # Multiple optimizers (e.g.: GANs)\n",
            "    def training_step(self, batch, batch_idx):\n",
            "        opt1, opt2 = self.optimizers()\n",
            "\n",
            "        # do training_step with encoder\n",
            "        ...\n",
            "        opt1.step()\n",
            "        # do training_step with decoder\n",
            "        ...\n",
            "        opt2.step()\n",
            "\n",
            "Note:\n",
            "    When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically\n",
            "    normalized by ``accumulate_grad_batches`` internally.\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "    \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      ~/development/research/FSDL/fsdl-text-recognizer-2022-labs_understanding/lab02/text_recognizer/lit_models/base.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "BaseLitModel.training_step??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhoYf_NoEyc"
      },
      "source": [
        "#### `.configure_optimizers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCIAWoCEtIU7"
      },
      "source": [
        "Thanks to `training_step` we've got a loss, and PyTorch can turn that into a gradient.\n",
        "\n",
        "But we need more than a gradient to do an update.\n",
        "\n",
        "We need an _optimizer_ that can make use of the gradients to update the parameters. In complex cases, we might need more than one optimizer (e.g. GANs).\n",
        "\n",
        "Our second required method, `.configure_optimizers`,\n",
        "sets up the `torch.optim.Optimizer`s \n",
        "(e.g. setting their hyperparameters\n",
        "and pointing them at the `Module`'s parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMlnRdIPzvDF"
      },
      "source": [
        "In psuedo-code (modified from the Lightning documentation), it gets used something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBnfJzszi49"
      },
      "source": [
        "```python\n",
        "optimizer = model.configure_optimizers()\n",
        "\n",
        "for batch_idx, batch in enumerate(data):\n",
        "\n",
        "    def closure():  # wrap the loss calculation\n",
        "        loss = model.training_step(batch, batch_idx, ...)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    # optimizer can call the loss calculation as many times as it likes\n",
        "    optimizer.step(closure)  # some optimizers need this, like (L)-BFGS\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsP3DBy7YzW"
      },
      "source": [
        "For our `LinearRegression` model,\n",
        "we just need to instantiate an optimizer and point it at the parameters of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZWrWGgdVt21h"
      },
      "outputs": [],
      "source": [
        "def configure_optimizers(self: LinearRegression) -> torch.optim.Optimizer:\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)  # https://fsdl.me/ol-reliable-img\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "LinearRegression.configure_optimizers = configure_optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta2hs0OLwbtF"
      },
      "source": [
        "You can read more about optimization in Lightning,\n",
        "including how to manually control optimization\n",
        "instead of relying on default behavior,\n",
        "in the docs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KXINqlAgwfKy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/2.4.0/common/optimization.html'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimization_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/optimization.html\"\n",
        "optimization_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWdKdZDfxmb2"
      },
      "source": [
        "The `configure_optimizers` method for the `BaseLitModel`\n",
        "isn't that much more complex.\n",
        "\n",
        "We just add support for learning rate schedulers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kyRbz0bEpWwd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mBaseLitModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m\n",
            "Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\n",
            "But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\n",
            "the manual optimization mode.\n",
            "\n",
            "Return:\n",
            "    Any of these 6 options.\n",
            "\n",
            "    - **Single optimizer**.\n",
            "    - **List or Tuple** of optimizers.\n",
            "    - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\n",
            "      (or multiple ``lr_scheduler_config``).\n",
            "    - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\n",
            "      key whose value is a single LR scheduler or ``lr_scheduler_config``.\n",
            "    - **None** - Fit will run without any optimizer.\n",
            "\n",
            "The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\n",
            "The default configuration is shown below.\n",
            "\n",
            ".. code-block:: python\n",
            "\n",
            "    lr_scheduler_config = {\n",
            "        # REQUIRED: The scheduler instance\n",
            "        \"scheduler\": lr_scheduler,\n",
            "        # The unit of the scheduler's step size, could also be 'step'.\n",
            "        # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
            "        # updates it after a optimizer update.\n",
            "        \"interval\": \"epoch\",\n",
            "        # How many epochs/steps should pass between calls to\n",
            "        # `scheduler.step()`. 1 corresponds to updating the learning\n",
            "        # rate after every epoch/step.\n",
            "        \"frequency\": 1,\n",
            "        # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
            "        \"monitor\": \"val_loss\",\n",
            "        # If set to `True`, will enforce that the value specified 'monitor'\n",
            "        # is available when the scheduler is updated, thus stopping\n",
            "        # training if not found. If set to `False`, it will only produce a warning\n",
            "        \"strict\": True,\n",
            "        # If using the `LearningRateMonitor` callback to monitor the\n",
            "        # learning rate progress, this keyword can be used to specify\n",
            "        # a custom logged name\n",
            "        \"name\": None,\n",
            "    }\n",
            "\n",
            "When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\n",
            ":class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\n",
            "``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\n",
            "should be conditioned on.\n",
            "\n",
            ".. testcode::\n",
            "\n",
            "    # The ReduceLROnPlateau scheduler requires a monitor\n",
            "    def configure_optimizers(self):\n",
            "        optimizer = Adam(...)\n",
            "        return {\n",
            "            \"optimizer\": optimizer,\n",
            "            \"lr_scheduler\": {\n",
            "                \"scheduler\": ReduceLROnPlateau(optimizer, ...),\n",
            "                \"monitor\": \"metric_to_track\",\n",
            "                \"frequency\": \"indicates how often the metric is updated\",\n",
            "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
            "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
            "            },\n",
            "        }\n",
            "\n",
            "\n",
            "    # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\n",
            "    def configure_optimizers(self):\n",
            "        optimizer1 = Adam(...)\n",
            "        optimizer2 = SGD(...)\n",
            "        scheduler1 = ReduceLROnPlateau(optimizer1, ...)\n",
            "        scheduler2 = LambdaLR(optimizer2, ...)\n",
            "        return (\n",
            "            {\n",
            "                \"optimizer\": optimizer1,\n",
            "                \"lr_scheduler\": {\n",
            "                    \"scheduler\": scheduler1,\n",
            "                    \"monitor\": \"metric_to_track\",\n",
            "                },\n",
            "            },\n",
            "            {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\n",
            "        )\n",
            "\n",
            "Metrics can be made available to monitor by simply logging it using\n",
            "``self.log('metric_to_track', metric_val)`` in your :class:`~pytorch_lightning.core.LightningModule`.\n",
            "\n",
            "Note:\n",
            "    Some things to know:\n",
            "\n",
            "    - Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\n",
            "    - If a learning rate scheduler is specified in ``configure_optimizers()`` with key\n",
            "      ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\n",
            "      the scheduler's ``.step()`` method automatically in case of automatic optimization.\n",
            "    - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\n",
            "    - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\n",
            "    - If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\n",
            "      yourself.\n",
            "    - If you need to control how often the optimizer steps, override the :meth:`optimizer_step` hook.\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "    \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_max_lr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneCycleLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_max_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_total_steps\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr_scheduler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"monitor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"validation/loss\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      ~/development/research/FSDL/fsdl-text-recognizer-2022-labs_understanding/lab02/text_recognizer/lit_models/base.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "BaseLitModel.configure_optimizers??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQCfn7Nm_QP"
      },
      "source": [
        "# The `pl.Trainer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RScc0ef97qlc"
      },
      "source": [
        "The `LightningModule` has already helped us organize our code,\n",
        "but it's not really useful until we combine it with the `Trainer`,\n",
        "which relies on the `LightningModule` interface to execute training, validation, and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBdikPBF86Qp"
      },
      "source": [
        "The `Trainer` is where we make choices like how long to train\n",
        "(`max_epochs`, `min_epochs`, `max_time`, `max_steps`),\n",
        "what kind of acceleration (e.g. `gpus`) or distribution strategy to use,\n",
        "and other settings that might differ across training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ4KSdFP3E4Q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=20,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1 if torch.cuda.is_available() else \"auto\"\n",
        ") #Simple alternative - trainer = pl.Trainer(max_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2l3rGZK7-PL"
      },
      "source": [
        "Before we can actually use the `Trainer`, though,\n",
        "we also need a `torch.utils.data.DataLoader` --\n",
        "nothing new from PyTorch Lightning here,\n",
        "just vanilla PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OcUSD2jP4Ffo"
      },
      "outputs": [],
      "source": [
        "class CorrelatedDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, N=10_000):\n",
        "        self.N = N\n",
        "        self.xs = torch.randn(size=(N, 1))\n",
        "        self.ys = torch.randn_like(self.xs) + self.xs  # correlated target data: y ~ N(x, 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.xs[idx], self.ys[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "dataset = CorrelatedDataset()\n",
        "tdl = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0u41JtA8qGo"
      },
      "source": [
        "We can fetch some sample data from the `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "z1j6Gj9Ka0dJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs:\n",
            "tensor([[-0.3994],\n",
            "        [ 2.0797],\n",
            "        [ 0.4197],\n",
            "        [-0.5436],\n",
            "        [ 0.4251],\n",
            "        [-1.0875],\n",
            "        [ 0.6046],\n",
            "        [-0.2495],\n",
            "        [-0.0868],\n",
            "        [ 1.0742]])\n",
            "ys:\n",
            "tensor([[-0.6221],\n",
            "        [ 4.4763],\n",
            "        [ 1.7724],\n",
            "        [-0.3509],\n",
            "        [ 1.7796],\n",
            "        [-0.8583],\n",
            "        [-0.5254],\n",
            "        [ 0.2554],\n",
            "        [ 1.3844],\n",
            "        [ 0.5908]])\n"
          ]
        }
      ],
      "source": [
        "example_xs, example_ys = next(iter(tdl))  # grabbing an example batch to print\n",
        "\n",
        "print(\"xs:\", example_xs[:10], sep=\"\\n\")\n",
        "print(\"ys:\", example_ys[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnqk3mRv8dbW"
      },
      "source": [
        "and, since it's low-dimensional, visualize it\n",
        "and see what we're asking the model to learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "33jcHbErbl6Q"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
        "  .plot(x=\"x\", y=\"y\", kind=\"scatter\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA7-4tJJ9fde"
      },
      "source": [
        "Now we're ready to run training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IY910O803oPU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a CUDA device ('NVIDIA GeForce RTX 4080 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss before training: 1.1291513442993164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type   | Params | Mode \n",
            "-----------------------------------------\n",
            "0 | model | Linear | 2      | train\n",
            "-----------------------------------------\n",
            "2         Trainable params\n",
            "0         Non-trainable params\n",
            "2         Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "1         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/home/suhdas/miniconda3/envs/TSTCC_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91b5e17af85640be8b5b4dd3ae9e4a58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after training: 1.0022704601287842\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
        "\n",
        "trainer.fit(model=model, train_dataloaders=tdl)\n",
        "\n",
        "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQBXYmLF_GoI"
      },
      "source": [
        "The loss after training should be less than the loss before training,\n",
        "and we can see that our model's predictions line up with the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jqcbA91x96-s"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
        "  .plot(x=\"x\", y=\"y\", legend=True, kind=\"scatter\", label=\"data\")\n",
        "\n",
        "inps = torch.arange(-2, 2, 0.5)[:, None]\n",
        "ax.plot(inps, model(inps).detach(), lw=2, color=\"k\", label=\"predictions\"); ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZkpsNfl3P8R"
      },
      "source": [
        "The `Trainer` promises to \"customize every aspect of training via flags\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_Q-c9b62_XFj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Customize every aspect of training via flags.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.Trainer.__init__.__doc__.strip().split(\"\\n\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He-zEwMB_oKH"
      },
      "source": [
        "and they mean _every_ aspect.\n",
        "\n",
        "The cell below prints all of the arguments for the `pl.Trainer` class --\n",
        "no need to memorize or even understand them all now,\n",
        "just skim it to see how many customization options there are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8F_rRPL3lfPE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customize every aspect of training via flags.\n",
            "\n",
            "        Args:\n",
            "            accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\n",
            "                as well as custom accelerator instances.\n",
            "\n",
            "            strategy: Supports different training strategies with aliases as well custom strategies.\n",
            "                Default: ``\"auto\"``.\n",
            "\n",
            "            devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
            "                (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
            "                automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
            "\n",
            "            num_nodes: Number of GPU nodes for distributed training.\n",
            "                Default: ``1``.\n",
            "\n",
            "            precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
            "                16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
            "                Can be used on CPU, GPU, TPUs, or HPUs.\n",
            "                Default: ``'32-true'``.\n",
            "\n",
            "            logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
            "                the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
            "                ``False`` will disable logging. If multiple loggers are provided, local files\n",
            "                (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n",
            "                Default: ``True``.\n",
            "\n",
            "            callbacks: Add a callback or list of callbacks.\n",
            "                Default: ``None``.\n",
            "\n",
            "            fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
            "                of train, val and test to find any bugs (ie: a sort of unit test).\n",
            "                Default: ``False``.\n",
            "\n",
            "            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
            "                If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
            "                To enable infinite training, set ``max_epochs = -1``.\n",
            "\n",
            "            min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
            "\n",
            "            max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
            "                and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
            "                ``max_epochs`` to ``-1``.\n",
            "\n",
            "            min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
            "\n",
            "            max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
            "                The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
            "                :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
            "                :class:`datetime.timedelta`.\n",
            "\n",
            "            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
            "                Default: ``0.0``.\n",
            "\n",
            "            val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
            "                after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
            "                batches. An ``int`` value can only be higher than the number of training batches when\n",
            "                ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
            "                across epochs or during iteration-based training.\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\n",
            "                validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
            "                to be an integer value.\n",
            "                Default: ``1``.\n",
            "\n",
            "            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
            "                Set it to `-1` to run all batches in all validation dataloaders.\n",
            "                Default: ``2``.\n",
            "\n",
            "            log_every_n_steps: How often to log within steps.\n",
            "                Default: ``50``.\n",
            "\n",
            "            enable_checkpointing: If ``True``, enable checkpointing.\n",
            "                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
            "                Default: ``True``.\n",
            "\n",
            "            enable_progress_bar: Whether to enable to progress bar by default.\n",
            "                Default: ``True``.\n",
            "\n",
            "            enable_model_summary: Whether to enable model summarization by default.\n",
            "                Default: ``True``.\n",
            "\n",
            "            accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
            "                Default: 1.\n",
            "\n",
            "            gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
            "                gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
            "                Default: ``None``.\n",
            "\n",
            "            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
            "                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
            "                be set to ``\"norm\"``.\n",
            "\n",
            "            deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
            "                Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
            "                that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\n",
            "\n",
            "            benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
            "                The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
            "                (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
            "                is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
            "                Default: ``None``.\n",
            "\n",
            "            inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
            "                evaluation (``validate``/``test``/``predict``).\n",
            "\n",
            "            use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
            "                :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
            "                strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
            "                ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
            "                ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
            "                sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
            "                we don't do this automatically.\n",
            "\n",
            "            profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
            "                Default: ``None``.\n",
            "\n",
            "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
            "                Default: ``False``.\n",
            "\n",
            "            barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
            "                disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
            "                runs. The following features are deactivated:\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_checkpointing`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.logger`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_progress_bar`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.log_every_n_steps`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_model_summary`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.fast_dev_run`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.detect_anomaly`,\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.profiler`,\n",
            "                :meth:`~pytorch_lightning.core.LightningModule.log`,\n",
            "                :meth:`~pytorch_lightning.core.LightningModule.log_dict`.\n",
            "            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
            "                Default: ``None``.\n",
            "\n",
            "            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
            "                Default: ``False``.\n",
            "\n",
            "            reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\n",
            "                Default: ``0``.\n",
            "\n",
            "            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
            "                Default: ``os.getcwd()``.\n",
            "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
            "\n",
            "        Raises:\n",
            "            TypeError:\n",
            "                If ``gradient_clip_val`` is not an int or float.\n",
            "\n",
            "            MisconfigurationException:\n",
            "                If ``gradient_clip_algorithm`` is invalid.\n",
            "\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(pl.Trainer.__init__.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X8dGmR53kYU"
      },
      "source": [
        "It's probably easier to read them on the documentation website:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cqUj6MxRkppr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/2.4.0/common/trainer.html'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_docs_link = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/trainer.html\"\n",
        "trainer_docs_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T8XMYvr__Y5"
      },
      "source": [
        "# Training with PyTorch Lightning in the FSDL Codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CtaPliTAxy3"
      },
      "source": [
        "The `LightningModule`s in the FSDL codebase\n",
        "are stored in the `lit_models` submodule of the `text_recognizer` module.\n",
        "\n",
        "For now, we've just got some basic models.\n",
        "We'll add more as we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NMe5z1RSAyo_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py  __pycache__  base.py\n"
          ]
        }
      ],
      "source": [
        "!ls text_recognizer/lit_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZTYmIHbBu7g"
      },
      "source": [
        "We also have a folder called `training` now.\n",
        "\n",
        "This contains a script, `run_experiment.py`,\n",
        "that is used for running training jobs.\n",
        "\n",
        "In case you want to play around with the training code\n",
        "in a notebook, you can also load it as a module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DRz9GbXzNJLM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py  run_experiment.py\tutil.py\n"
          ]
        }
      ],
      "source": [
        "!ls training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Im9vLeyqBv_h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment-running framework. \n",
            "    Run an experiment.\n",
            "\n",
            "    Sample command:\n",
            "    ```\n",
            "    python training/run_experiment.py --max_epochs=3 --gpus='0,' --num_workers=20 --model_class=MLP --data_class=MNIST\n",
            "    ```\n",
            "\n",
            "    For basic help documentation, run the command\n",
            "    ```\n",
            "    python training/run_experiment.py --help\n",
            "    ```\n",
            "\n",
            "    The available command line args differ depending on some of the arguments, including --model_class and --data_class.\n",
            "\n",
            "    To see which command line args are available and read their documentation, provide values for those arguments\n",
            "    before invoking --help, like so:\n",
            "    ```\n",
            "    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import training.run_experiment\n",
        "\n",
        "\n",
        "print(training.run_experiment.__doc__, training.run_experiment.main.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hcAXqHAV0v"
      },
      "source": [
        "We build the `Trainer` from command line arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi50CDZul7Mm"
      },
      "outputs": [],
      "source": [
        "# how the trainer is initialized in the training script\n",
        "!grep \"pl.Trainer.from\" training/run_experiment.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQheYJyAxlh"
      },
      "source": [
        "so all the configuration flexibility and complexity of the `Trainer`\n",
        "is available via the command line.\n",
        "\n",
        "Docs for the command line arguments for the trainer are accessible with `--help`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlSmSyCMAw7Z"
      },
      "outputs": [],
      "source": [
        "# displays the first few flags for controlling the Trainer from the command line\n",
        "!python training/run_experiment.py --help | grep \"pl.Trainer\" -A 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIZ_VRPcNMsM"
      },
      "source": [
        "We'll use `run_experiment` in\n",
        "[Lab 02b](http://fsdl.me/lab02b-colab)\n",
        "to train convolutional neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0siaL4Qumc_"
      },
      "source": [
        "# Extra Goodies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQSPnxQDBF6"
      },
      "source": [
        "The `LightningModule` and the `Trainer` are the minimum amount you need\n",
        "to get started with PyTorch Lightning.\n",
        "\n",
        "But they aren't all you need.\n",
        "\n",
        "There are many more features built into Lightning and its ecosystem.\n",
        "\n",
        "We'll cover three more here:\n",
        "- `pl.LightningDataModule`s, for organizing dataloaders and handling data in distributed settings\n",
        "- `pl.Callback`s, for adding \"optional\" extra features to model training\n",
        "- `torchmetrics`, for efficiently computing and logging "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOYHSLw_D8Zy"
      },
      "source": [
        "## `pl.LightningDataModule`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjTNGzREIpl"
      },
      "source": [
        "Where the `LightningModule` organizes our model and its optimizers,\n",
        "the `LightningDataModule` organizes our dataloading code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_KkQ0iOWKD7"
      },
      "source": [
        "The class-level docstring explains the concept\n",
        "behind the class well\n",
        "and lists the main methods to be over-ridden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFTWHdsFV5WG"
      },
      "outputs": [],
      "source": [
        "print(pl.LightningDataModule.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiacppGB9BB"
      },
      "source": [
        "Let's upgrade our `CorrelatedDataset` from a PyTorch `Dataset` to a `LightningDataModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1d62iC6Xv1i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class CorrelatedDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, size=10_000, train_frac=0.8, batch_size=32):\n",
        "        super().__init__()  # again, mandatory superclass init, as with torch.nn.Modules\n",
        "\n",
        "        # set some constants, like the train/val split\n",
        "        self.size = size\n",
        "        self.train_frac, self.val_frac = train_frac, 1 - train_frac\n",
        "        self.train_indices = list(range(math.floor(self.size * train_frac)))\n",
        "        self.val_indices = list(range(self.train_indices[-1], self.size))\n",
        "\n",
        "        # under the hood, we've still got a torch Dataset\n",
        "        self.dataset = CorrelatedDataset(N=size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQf-jUYRCi3m"
      },
      "source": [
        "`LightningDataModule`s are designed to work in distributed settings,\n",
        "where operations that set state\n",
        "(e.g. writing to disk or attaching something to `self` that you want to access later)\n",
        "need to be handled with care.\n",
        "\n",
        "Getting data ready for training is often a very stateful operation,\n",
        "so the `LightningDataModule` provides two separate methods for it:\n",
        "one called `setup` that handles any state that needs to be set up in each copy of the module\n",
        "(here, splitting the data and adding it to `self`)\n",
        "and one called `prepare_data` that handles any state that only needs to be set up in each machine\n",
        "(for example, downloading data from storage and writing it to the local disk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mttu--rHX70r"
      },
      "outputs": [],
      "source": [
        "def setup(self, stage=None):  # prepares state that needs to be set for each GPU on each node\n",
        "    if stage == \"fit\" or stage is None:  # other stages: \"test\", \"predict\"\n",
        "        self.train_dataset = torch.utils.data.Subset(self.dataset, self.train_indices)\n",
        "        self.val_dataset = torch.utils.data.Subset(self.dataset, self.val_indices)\n",
        "\n",
        "def prepare_data(self):  # prepares state that needs to be set once per node\n",
        "    pass  # but we don't have any \"node-level\" computations\n",
        "\n",
        "\n",
        "CorrelatedDataModule.setup, CorrelatedDataModule.prepare_data = setup, prepare_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh3mZrjwD83Y"
      },
      "source": [
        "We then define methods to return `DataLoader`s when requested by the `Trainer`.\n",
        "\n",
        "To run a testing loop that uses a `LightningDataModule`,\n",
        "you'll also need to define a `test_dataloader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu9Ma3iKYPBd"
      },
      "outputs": [],
      "source": [
        "def train_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "    return torch.utils.data.DataLoader(self.train_dataset, batch_size=32)\n",
        "\n",
        "def val_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "    return torch.utils.data.DataLoader(self.val_dataset, batch_size=32)\n",
        "\n",
        "CorrelatedDataModule.train_dataloader, CorrelatedDataModule.val_dataloader = train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNodiN6oawX5"
      },
      "source": [
        "Now we're ready to run training using a datamodule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKBwoE-Rajqw"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "trainer.fit(model=model, datamodule=datamodule)\n",
        "\n",
        "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6flh5Jf2ZP"
      },
      "source": [
        "Notice the warning: \"`Skipping val loop.`\"\n",
        "\n",
        "It's being raised because our minimal `LinearRegression` model\n",
        "doesn't have a `.validation_step` method.\n",
        "\n",
        "In the exercises, you're invited to add a validation step and resolve this warning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJnoFx47ZjBw"
      },
      "source": [
        "In the FSDL codebase,\n",
        "we define the basic functions of a `LightningDataModule`\n",
        "in the `BaseDataModule` and defer details to subclasses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTPKvDDGXmOr"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.data import BaseDataModule\n",
        "\n",
        "\n",
        "BaseDataModule??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mRlZecwaKB4"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.data.mnist import MNIST\n",
        "\n",
        "\n",
        "MNIST??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQbMY08qD-hm"
      },
      "source": [
        "## `pl.Callback`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVe7TSNvHK4K"
      },
      "source": [
        "Lightning's `Callback` class is used to add \"nice-to-have\" features\n",
        "to training, validation, and testing\n",
        "that aren't strictly necessary for any model to run\n",
        "but are useful for many models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzU76wgFGw9N"
      },
      "source": [
        "A \"callback\" is a unit of code that's meant to be called later,\n",
        "based on some trigger.\n",
        "\n",
        "It's a very flexible system, which is why\n",
        "`Callback`s are used internally to implement lots of important Lightning features,\n",
        "including some we've already discussed, like `ModelCheckpoint` for saving during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-msDjbKdHTxU"
      },
      "outputs": [],
      "source": [
        "pl.callbacks.__all__  # builtin Callbacks from Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6WRNXtHHkbM"
      },
      "source": [
        "The triggers, or \"hooks\", here, are specific points in the training, validation, and testing loop.\n",
        "\n",
        "The names of the hooks generally explain when the hook will be called,\n",
        "but you can always check the documentation for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iHjjnU8Hvgg"
      },
      "outputs": [],
      "source": [
        "hooks = \", \".join([method for method in dir(pl.Callback) if method.startswith(\"on_\")])\n",
        "print(\"hooks:\", *textwrap.wrap(hooks, width=80), sep=\"\\n\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E2M7O2cGdj7"
      },
      "source": [
        "You can define your own `Callback` by inheriting from `pl.Callback`\n",
        "and over-riding one of the \"hook\" methods --\n",
        "much the same way that you define your own `LightningModule`\n",
        "by writing your own `.training_step` and `.configure_optimizers`.\n",
        "\n",
        "Let's define a silly `Callback` just to demonstrate the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UodFQKAGEJlk"
      },
      "outputs": [],
      "source": [
        "class HelloWorldCallback(pl.Callback):\n",
        "\n",
        "    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        print(\"👋 hello from the start of the training epoch!\")\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        print(\"👋 hello from the end of the validation epoch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7oIpyEGoaP"
      },
      "source": [
        "This callback will print a message whenever the training epoch starts\n",
        "and whenever the validation epoch ends.\n",
        "\n",
        "Different \"hooks\" have different information directly available.\n",
        "\n",
        "For example, you can directly access the batch information\n",
        "inside the `on_train_batch_start` and `on_train_batch_end` hooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U17Qo_i_GCya"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def on_train_batch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
        "        if random.random() > 0.995:\n",
        "            print(f\"👋 hello from inside the lucky batch, #{batch_idx}!\")\n",
        "\n",
        "\n",
        "HelloWorldCallback.on_train_batch_start = on_train_batch_start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVKQXZOwQNGJ"
      },
      "source": [
        "We provide the callbacks when initializing the `Trainer`,\n",
        "then they are invoked during model fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XHXZ64-ETCz"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "trainer = pl.Trainer(  # we instantiate and provide the callback here, but nothing happens yet\n",
        "    max_epochs=10, gpus=int(torch.cuda.is_available()), callbacks=[HelloWorldCallback()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEHUUhVOQv6K"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP2Xj1woFGwG"
      },
      "source": [
        "You can read more about callbacks in the documentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COHk5BZvFJN_"
      },
      "outputs": [],
      "source": [
        "callback_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/extensions/callbacks.html\"\n",
        "callback_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2K9e44iEGCR"
      },
      "source": [
        "## `torchmetrics`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO-UIFKyJCqJ"
      },
      "source": [
        "DNNs are also finicky and break silently:\n",
        "rather than crashing, they just start doing the wrong thing.\n",
        "Without careful monitoring, that wrong thing can be invisible\n",
        "until long after it has done a lot of damage to you, your team, or your users.\n",
        "\n",
        "We want to calculate metrics so we can monitor what's happening during training and catch bugs --\n",
        "or even achieve [\"observability\"](https://thenewstack.io/observability-a-3-year-retrospective/),\n",
        "meaning we can also determine\n",
        "how to fix bugs in training just by viewing logs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4YMyUI0Jr2f"
      },
      "source": [
        "But DNN training is also performance sensitive.\n",
        "Training runs for large language models have budgets that are\n",
        "more comparable to building an apartment complex\n",
        "than they are to the build jobs of traditional software pipelines.\n",
        "\n",
        "Slowing down training even a small amount can add a substantial dollar cost,\n",
        "obviating the benefits of catching and fixing bugs more quickly.\n",
        "\n",
        "Also implementing metric calculation during training adds extra work,\n",
        "much like the other software engineering best practices which it closely resembles,\n",
        "namely test-writing and monitoring.\n",
        "This distracts and detracts from higher-leverage research work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvWjiHSIxzM"
      },
      "source": [
        "\n",
        "The `torchmetrics` library, which began its life as `pytorch_lightning.metrics`,\n",
        "resolves these issues by providing a `Metric` class that\n",
        "incorporates best performance practices,\n",
        "like smart accumulation across batches and over devices,\n",
        "defines a unified interface,\n",
        "and integrates with Lightning's built-in logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21y3lgvwEKPC"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "\n",
        "tm_version = torchmetrics.__version__\n",
        "print(\"metrics:\", *textwrap.wrap(\", \".join(torchmetrics.__all__), width=80), sep=\"\\n\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TuPZkV1gfFE"
      },
      "source": [
        "Like the `LightningModule`, `torchmetrics.Metric` inherits from `torch.nn.Module`.\n",
        "\n",
        "That's because metric calculation, like module application, is typically\n",
        "1) an array-heavy computation that\n",
        "2) relies on persistent state\n",
        "(parameters for `Module`s, running values for `Metric`s) and\n",
        "3) benefits from acceleration and\n",
        "4) can be distributed over devices and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leiiI_QDS2_V"
      },
      "outputs": [],
      "source": [
        "issubclass(torchmetrics.Metric, torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy8MF2taP8MV"
      },
      "source": [
        "Documentation for the version of `torchmetrics` we're using can be found here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN4ashooP_tM"
      },
      "outputs": [],
      "source": [
        "torchmetrics_docs_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/\"\n",
        "torchmetrics_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aycHhZNXwjr"
      },
      "source": [
        "In the `BaseLitModel`,\n",
        "we use the `torchmetrics.Accuracy` metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyq4IjmBXzTv"
      },
      "outputs": [],
      "source": [
        "BaseLitModel.__init__??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPoTH50YfkMF"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD_6PVAeflWw"
      },
      "source": [
        "### 🌟 Add a `validation_step` to the `LinearRegression` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KKbAN9eK281"
      },
      "outputs": [],
      "source": [
        "def validation_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    pass # your code here\n",
        "\n",
        "\n",
        "LinearRegression.validation_step = validation_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnPPHAPxFCEv"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "# if you code is working, you should see results for the validation loss in the output\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u42zXktOFDhZ"
      },
      "source": [
        "### 🌟🌟 Add a `test_step` to the `LinearRegression` class and a `test_dataloader` to the `CorrelatedDataModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbWfqvumFESV"
      },
      "outputs": [],
      "source": [
        "def test_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    pass # your code here\n",
        "\n",
        "LinearRegression.test_step = test_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB96MpibLeJi"
      },
      "outputs": [],
      "source": [
        "class CorrelatedDataModuleWithTest(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, N=10_000, N_test=10_000):  # reimplement __init__ here\n",
        "        super().__init__()  # don't forget this!\n",
        "        self.dataset = None\n",
        "        self.test_dataset = None  # define a test set -- another sample from the same distribution\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        pass\n",
        "\n",
        "    def test_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "        pass  # create a dataloader for the test set here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jq3dcugMMOu"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModuleWithTest()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# we run testing without fitting here\n",
        "trainer.test(model=model, datamodule=datamodule)  # if your code is working, you should see performance on the test set here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHg4MKmJPla6"
      },
      "source": [
        "### 🌟🌟🌟 Make a version of the `LinearRegression` class that calculates the `ExplainedVariance` metric during training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_1AKGWRR2ai"
      },
      "source": [
        "The \"variance explained\" is a useful metric for comparing regression models --\n",
        "its values are interpretable and comparable across datasets, unlike raw loss values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLecK4CsQWKk"
      },
      "source": [
        "Read the \"TorchMetrics in PyTorch Lightning\" guide for details on how to\n",
        "add metrics and metric logging\n",
        "to a `LightningModule`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWy0HyG4RYnX"
      },
      "outputs": [],
      "source": [
        "torchmetrics_guide_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/pages/lightning.html\"\n",
        "torchmetrics_guide_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoSQ3y6sSTvP"
      },
      "source": [
        "And check out the docs for `ExplainedVariance` to see how it's calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpGuRK2FRHh1"
      },
      "outputs": [],
      "source": [
        "print(torchmetrics.ExplainedVariance.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EAtpWXrSVR1"
      },
      "source": [
        "You'll want to start the `LinearRegression` class over from scratch,\n",
        "since the `__init__` and `{training, validation, test}_step` methods need to be rewritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGtWt3_5SYTn"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFWNr1SfS5-r"
      },
      "source": [
        "You can test your code by running fitting and testing.\n",
        "\n",
        "To see whether it's working,\n",
        "[call `self.log` inside the `_step` methods](https://torchmetrics.readthedocs.io/en/v0.7.1/pages/lightning.html)\n",
        "with the\n",
        "[keyword argument `prog_bar=True`](https://pytorch-lightning.readthedocs.io/en/1.6.1/api/pytorch_lightning.core.LightningModule.html#pytorch_lightning.core.LightningModule.log).\n",
        "You should see the explained variance show up in the output alongside the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jse95DGCS6gR",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# if your code is working, you should see explained variance in the progress bar/logs\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab02a_lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "TSTCC_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
